<!DOCTYPE HTML>
<!--
	Dimension by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Kelvin's Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<nav>
							<ul>
								<li><a href="#about">About</a></li>
								<li><a href="#contact">Contact</a></li>
								<!--<li><a href="#elements">Elements</a></li>-->
							</ul>
							
						</nav>
						<div class="content">
							<div class="inner">
								<h1>Kelvin's Portfolio</h1>
								<p>Thank you for checking out my website and showing interest in some of my projects/work!</p>
							</div>
						</div>
						<nav>
							<ul>
								<li><a href="#KoreanNLP">Korean NLP - Film Analysis</a></li>
								<li><a href="#FederalTimeSeries">Federal Salary Time Series Forecast</a></li>
								<li><a href="#ImageColorization">Colorization of Black/White Images</a></li>
								<!--<li><a href="#elements">Elements</a></li>-->
							</ul>
						</nav>

					</header>

				<!-- Main -->
					<div id="main">

						<!-- Intro -->
							<article id="intro">
								<h2 class="major">Intro</h2>
								<span class="image main"><img src="images/pic01.jpg" alt="" /></span>
								<p>Since my undergrad days, I have had a lot of interest in various different fields. I enjoyed scripting, statistics, product design, and project management. I have been fortunate enough to work in various industries including defense, tech, medical, and automotive to gain wide exposure to diffent fields and diciplines. I took on roles where I was able to combine my knowledge of hardware design, data analytics, and scripting. During my internship at Tesla, I was able to script Tesla’s steerwheel test automation and also took part in its handle design. For my Master’s, I worked on new product development for client companies by utilizing machine learning algorithms to determine optimal design parameters. Currently, I work at AMD where I created a database of designed heatsink specs and created a forecast model to estimate future costs and design requirements. </p>
								<p>In the recent years, I have found myself to be surrounded by an abundance of data when it comes to design parameters, simulations, and production yield results. As a result, I found myself diving deeper and deeper into the field of data science and machine learning to incorporate my surrounding data into my work. I realized that I was having a lot of fun learning about the field, implementing it, and even doing some side projects. The whole process from data collection, refinement, model conception, and scripting was all highly enjoyable for me. When this clicked, I knew that I wanted to fully get into the field and that it would be the right choice for me going forward. </p>
								<p>I like to keep an eye out for upcoming techonlolgies and keeping up with the latest updates/trend in the industry. Growing up near the mountains in Calgary, I love to hike, snowboard, and skate. I enjoy going to salsa social clubs to dance and like to build computers in my spare time. </p>
							</article>

						<!-- Work -->
							<article id="work">
								<h2 class="major">Work</h2>
								<span class="image main"><img src="images/pic02.jpg" alt="" /></span>
								<p>Adipiscing magna sed dolor elit. Praesent eleifend dignissim arcu, at eleifend sapien imperdiet ac. Aliquam erat volutpat. Praesent urna nisi, fringila lorem et vehicula lacinia quam. Integer sollicitudin mauris nec lorem luctus ultrices.</p>
								<p>Nullam et orci eu lorem consequat tincidunt vivamus et sagittis libero. Mauris aliquet magna magna sed nunc rhoncus pharetra. Pellentesque condimentum sem. In efficitur ligula tate urna. Maecenas laoreet massa vel lacinia pellentesque lorem ipsum dolor. Nullam et orci eu lorem consequat tincidunt. Vivamus et sagittis libero. Mauris aliquet magna magna sed nunc rhoncus amet feugiat tempus.</p>
							</article>

						<!-- About -->
							<article id="about">
								<p>Hello there, welcome to my portfolio! My website is constantly being updated, so please feel free to check it out again from time to time! </p>
								<span class="image main"><img src="images/abou1.jpg" alt="" /></span>
								<p>I am an engineer with a background in data-driven product development. Since my undergrad days, I have had a lot of interest in various different fields. I enjoyed scripting, statistics, product design, and project management. I have been fortunate enough to work in various industries including defense, tech, medical, and automotive to gain wide exposure to diffent fields and diciplines. I took on roles where I was able to combine my knowledge of hardware design, data analytics, and scripting. During my internship at Tesla, I was able to script Tesla’s steerwheel test automation and also took part in its handle design. For my Master’s, I worked on new product development for client companies by utilizing machine learning algorithms to determine optimal design parameters. Currently, I work at AMD where I created a database of designed heatsink specs and created a forecast model to estimate future costs and design requirements. </p>
								<p>In the recent years, I have found myself to be surrounded by an abundance of data when it comes to design parameters, simulations, and production yield results. As a result, I found myself diving deeper and deeper into the field of data science and machine learning to incorporate my surrounding data into my work. I realized that I was having a lot of fun learning about the field, implementing it, and even doing some side projects. The whole process from data collection, refinement, model conception, and scripting was all highly enjoyable for me. When this clicked, I knew that I wanted to fully get into the field and that it would be the right choice for me going forward. </p>
								<p>I like to keep an eye out for upcoming techonlolgies and keeping up with the latest updates/trend in the industry. Growing up near the mountains in Calgary, I love to hike, snowboard, and skate. I enjoy going to salsa social clubs to dance and travelling around. </p>
								<span class="image main"><img src="images/abou3.jpg" alt="" /></span>

							</article>


						<!-- KoreanNLP -->
						<article id="KoreanNLP">
							<h2 class="major">Synopsis</h2>
							<p>Korean film industry has recently gathered international interest after the release of several high-profile films and shows in the last few years. Parasite, Okja, and Squid Games are some of the most well-received and highly consumed media originating from Korea. Fueled by this piqued interest in Korean media and my Korean heritage, I had several friends and co-workers asking for recommendations regarding movies and TV series. Unfortunately, as a Korean-Canadian who grew up playing hockey near the Rocky Mountains, I was pretty oblivious to the Korean popular culture despite my fluency in Korean. </p>
							<p>This inquiry and a Kaggle competition I saw regarding sentiment analysis of movie reviews motivated me to utilize Korean NLP. As my courses and online resources mainly dealt with English for NLP tutorials and exercises, I was highly curious how the model development process would differ when done in a different language. Plus, by doing a sentiment analysis of Korean movies using Korean reviews, I can have some good films I can recommend to my non-Korean friends and co-workers. Some similar older Korean film sentiment analysis exist, but it is my goal to take newer RNN architectures and try to further improve on the model’s. Also, I wished to take some different approaches in data pre-processing compared to existing works done. </p>	
							<h2 class="major">Background</h2>
							<p>Korean is an agglutinative language. This means that words are made up of a linear sequence of distinct morphemes. Morpheme is a short segment of languages that can be seen as the “smallest meaningful lexical item in a language”. It is basically the smallest possible unit of a language that has its own meaning. So, a Korean “word” can be compromised of several morphemes.  On the contrary, English is an isolating language. In a nutshell, each word typically consists of one morpheme. From the example below you can see each English word contains one morpheme. I, cognac, or drank cannot be split into a smaller lexical item. While Korean words 나는, 꼬냑을, and 들이켰다 consists of multiple morphemes. </p>
							<span class="image main"><img src="images/KoreanFilmNLP/KFNLP01.png" alt="" /></span>
							<p>This means that in English, tokenization is done via a word. However, in Korean, the morphemes used within that word needs to be considered as well. A lot of NLP development efforts in Korea involve proper refinement/building of the morpheme analyzer tool. I utilized the one listed here: https://konlpy.org/en/latest/references/#engines </p>
							<p>Although there are official language rules about when to use spaces in sentences. Most Korean do not properly follow the grammar on when/where to have spaces between the words. Part of this is because of the whole Morpheme/word system making the writer sometimes confused on the appropriate place to separate their word. </p>
							<h2 class="major">Dataset</h2>
							<p>I utilized an existing dataset for Korean film ratings along with their corresponding sentiment. The dataset was found from https://github.com/e9t/nsmc/. The review system rates films from 1-10 with 1 being the lowest rating. If reviewer gave the film 9 or 10, the corresponding sentiment was labeled with 1. If the reviewer gave the film 1~4, the corresponding sentiment was labeled with 0. Reviews from 5~8 were excluded from the dataset. The final dataset consists of 200000 reviews with a 75-25 split between the training and testing dataset. </p>
							<span class="image main"><img src="images/KoreanFilmNLP/KFNLP02.png" alt="" /></span>
							<p>From the imported data, it can be seen that there are 3 columns: user id, document (sentiment), and the label. As user id is useless, this column will be dropped from the dataset.</p>
							<span class="image main"><img src="images/KoreanFilmNLP/KFNLP03.png" alt="" /></span>
							<p>Looking at the high-level information about the data – there are 149995 non-null entries in the document (sentiment) column with none of the values in the label column being null. There are very minimal data missing from this overall dataset. There are no worries regarding duplicate reviews as in so many entries, it would be common to see duplicate sentiments/reviews especially if they are simple ones like “Good Movie”, “Recommend”, or “Horrible”. It can also be seen that the labels are object datatype. As there are only 5 empty values they will be dropped. </p>
							<span class="image main"><img src="images/KoreanFilmNLP/KFNLP04.png" alt="" /></span>
							<span class="image main"><img src="images/KoreanFilmNLP/KFNLP05.png" alt="" /></span>
							<p>Just like in English NLP data pre-processing, special characters and punctuation needs to be removed. Since this is a Korean language, the Korean character range of ㄱ-ㅎ ㅏ-ㅣ가-힣 was used. This range can be confirmed by looking at how they are listed in the Unicode chart. Thus, all characters excluding Korean and spaces were removed. It can be seen that punctuation were removed in the newly created column. </p>
							<span class="image main"><img src="images/KoreanFilmNLP/KFNLP06.png" alt="" /></span>
							<p>Korean stopwords are not supported by nltk.corpus; thus, they must be entered manually. This is combined with the tokenization from Open Korean Text Processor (OKT). This helps to simplified/unify the combinations/possibilities that arise from the morphemes as well.  From initial Tokenizing, there are more than 40000 tokens, the aim was to reduce that to less than 10000. Number of tokens were compared for each word occurrence threshold (in the dataset) required qualify as a token. From analysis, if a word occurred minimum 4 times in the dataset – it was tokenized. </p>
							<span class="image main"><img src="images/KoreanFilmNLP/KFNLP07.png" alt="" /></span>
							<p>Finally, the reviews must be padded/trimmed. Looking at the review length distribution graph – length of 50 words were chosen to minimize review cutoffs. </p>
							<span class="image main"><img src="images/KoreanFilmNLP/KFNLP08.png" alt="" /></span>	
							<p>The RNN architecture of the model used is briefly explained below. Inspiration of the architecture was found (https://www.kaggle.com/nilanml/imdb-review-deep-model-94-89-accuracy)</p>
							<ol>
								<li>Model is to start with an input layer from the tokenized dataset.</li>
								<li>The model is to utilize LSTM after its embedding layer. From looking at the dataset, the chaotic sentence structure of some reviews to greatly benefit from LSTM’s “context storage” capability. </li>
								<li>The model would then need to be flattened so that it can be fed into the dense layer.</li>
								<li>A dropout layer was added after that to reduce sample variance.</li>
								<li>As this is a binary classification problem, the model would need a logistic regression. As a result, sigmoid activation function was used as the final layer of the model.</li>
							</ol>
							<p>Hyperparameter tuning was conducted afterwards for accuracy optimization. The model architecture can be seen below. </p>
							<span class="image main"><img src="images/KoreanFilmNLP/KFNLP09.png" alt="" /></span>	
							<h2 class="major">Summary of Dataset</h2>
							<p>The model gave an accuracy of ~86%. This is an improvement of similar works done in the past utilizing NLP for Korean sentiments that saw an accuracy of 84~85%. The more modern utilization of LSTM compared to regular RNN/dense model is thought to have made this improvement possible. The model has a good distribution of its accuracy within the confusion matrix as well. </p>
							<span class="image main"><img src="images/KoreanFilmNLP/KFNLP10.png" alt="" /></span>
							<span class="image main"><img src="images/KoreanFilmNLP/KFNLP11.png" alt="" /></span>
							<p>Some English sentiment analysis models see accuracy >95% but due to differences in language/structure, Korean sentiment analysis models have yet to show such high accuracy. Also, due to the vast difference in the number of English speakers vs. Korean speakers, support tools for English NLP are much more polished with much more contributors (NLTK is much vaster and more developed than any current Korean language toolkit). However, I am optimistic that small, consistent improvements by numerous contributors will eventually allow Korean language models to also reach high levels of accuracy similar to its English counterpart. </p>
						</article>
						<!-- KoreanNLP -->
						<article id="FederalTimeSeries">	
							<h2 class="major">Synopsis</h2>		
							<p>With the market being volatile and inflation being unpredictable at times, I was interested in finding what the trend of a government worker’s payment would be. Talking with family and friends working in the government, they were stress-free about their income and job-stability as their pay does not get disturbed by the market and follows inflation. This was a topic of interest as the salary of a public servant would be a good benchmark to measure one’s compensation increase (assuming same position/seniority). </p>			
							<p>Another reason this was a topic of interest was that one can know if their pay is following the trend of the current economy and government worker’s payment would be a good to use as a comparison. Knowing the public sector salary is also important since they operate based on taxpayer money unlike corporations that are financed via private investors and revenue from capitalistic policies. Thus, it is crucial they are open and more accountable from taxpayers (Canadian Citizens) so that the general public knows that their tax money isn’t being mishandled and used in corrupt activities. Furthermore, it allows the comparing the performance of the federal government organizations with the compensation of its employees and using it to evaluate the efficiency of the government organization to other government</p>
							<h2 class="major">Background</h2>	
							<p>The reason behind why the Canadian Government started the project was to obtain data in employment. The data was collected by the Canadian Government via a survey and data collection from central payment system. The survey is a census with a cross-sectional design and data itself was collected for all the target population, thus meaning that no sampling was done. The data population is drawn from all institutes controlled and financed by the Canadian Government of all levels (federal, provincial, territorial, municipal). This includes each individual government departments, ministries, agencies, funds, crown corporations, public health institutions, and educational institutions.</p>
							<p>For the institutions that use a central pay system such as Phoenix Pay System, the data were obtained by referencing the system. For the entities not using such system, a questionnaire was developed as used for search purposes. One questionnaire was used for each of the federal institutional entities and provincial/territorial entities. The survey was also sent to some government business enterprises with no database. These questionnaires were designed to gather as much information on employment and salary.</p>
							<p>According to the statement released along with the data, the fluctuation in the time series data is due to seasonal, cyclic, and irregular fluctuations. The seasonal variation can be explained by common holiday and vacation period for workers, climate/weather affect crop yields, production and retail affected due to holidays such as Christmas. Thus, this would need to be accounted for in the data analysis.</p>
							<h2 class="major">Dataset</h2>	
							<span class="image main"><img src="images/FedTimeSeries/FTS1.png" alt="" /></span>
							<p>It can be seen clearly that there is seasonality involved with the data as the yearly highs generally happen around winter and summertime. There are two peaks in a year with one happening in the summer and one in the winter.  Although, there are some discrepancies as some years the peak happen in fall and spring, these only happen in the minority of the occurrences while in majority of the years, the peaks are predictable. Below shows the Box-and-Whisker graph that compares each month that can be used to further study each month.</p>
							<span class="image main"><img src="images/FedTimeSeries/FTS2.png" alt="" /></span>
							<p>For each month, the whiskers are the same length with the median splitting the box approximately in half. This implies the data in homoscedastic and normally distributed. Thus, box-cox transformation in not required to transform the data. As there is no other series or inputs for the data, the cross-correlation function is not required.</p>
							<span class="image main"><img src="images/FedTimeSeries/FTS3.png" alt="" /></span>
							<p>Figure above shows the overall trendline of the data. As the graph of the given time series does not seem to blur any statistical information to be shown and since seasonality and non-stationary nature of the graph can already be seen, Tukey Smoothing will not be needed for our analysis. Each season’s mean, std. dev, and variance was obtained.</p>
							<span class="image main"><img src="images/FedTimeSeries/FTS4.png" alt="" /></span>						
							<p>Looking at this data, the seasonality aspects are further noted. Summer months have a higher than average values along with December. The standard deviation value is used to obtain the ACF and generate the error bar for 95% confidence interval. </p>
							<span class="image main"><img src="images/FedTimeSeries/FTS5.png" alt="" /></span>								
							<p>From the figure, the ACF seems to die off even though it is quite slow in dying off. This indicates that differencing is strongly suggested. The prior graphs strongly indicated that the data is non-stationary just from a visual observation. The ACF further supports the notion that differencing is required. 1st order differencing and 2nd order differencing was conducted and their respective ACFs were generated. </p>
							<span class="image main"><img src="images/FedTimeSeries/FTS6.png" alt="" /></span>								
							<span class="image main"><img src="images/FedTimeSeries/FTS7.png" alt="" /></span>		
							<span class="image main"><img src="images/FedTimeSeries/FTS8.png" alt="" /></span>								
							<span class="image main"><img src="images/FedTimeSeries/FTS9.png" alt="" /></span>											
							<p>The figures show us that there isn’t much difference whether the data was differenced once or twice. The amount of lag that falls outside of the error bar does not decrease with the additional differencing. Therefore, the differencing will be done once in the base ARIMA model as ARIMA (P, 1, Q). The differencing also has made the data stationary compared to the original graph. Even though with the non-seasonal differencing, a lot of the values are above the significant levels. Thus, normal ARIMA model will not be enough by itself, and seasonality will have to be accounted for. Earlier, it was determined that the Box-Cox transformation would not be necessary so the value of λ is set as 1. With this, other parameters in the model can now be determined. SARIMA model looks like following:</p>
							<center>
								<p>(p, d, q) X (P, D, Q) <sub>S</sub></p>
								<p>(p, 1, q) X (P, 1, Q) <sub>12</sub></p>
							</center>	
							<p>Next, the PACF, IACF, IPACF of the differenced data was obtained and the following conclusions were made:</p>	
							<li>There are significantly large values of sample ACF and IPACF at sample at lag 12 and few other seasonal spikes, seasonal MA parameter is going to be needed.</li>
							<li>Sample IACF attenuates after first few lags, non-seasonal MA is needed. </li>
							<li>ACF seems to get cut off, non-seasonal AR parameter may not be needed. However, the IPACF has a large spike at lag 1 followed by a decreasing wave that fluctuates between the positive and negative values meaning that non-seasonal AR parameters may be required. </li>
							<li>Seasonal spikes in IPACF attenuates, seasonal AR is required. </li>
							<p></p>
							<p>The maximum likelihood estimations were obtained via Minitab software and was generated for each set of parameters. K value was generated for each combination by adding up the number of parameters.  </p>
							<span class="image main"><img src="images/FedTimeSeries/FTS10.png" alt="" /></span>											
							<h2 class="major">Summary of Dataset</h2>	
							<p>From looking at the AIC, (4,1,4)  X (2,1,2)<sub>12</sub> model was chosen as the most optimal to use as it had the lowest AIC value. The parameters obtained using maximum likelihood estimations as well. RACF of this model was graphed below. Along with the forecast of the model with their 95% confidence intervals. </p>
							<span class="image main"><img src="images/FedTimeSeries/FTS11.png" alt="" /></span>											
							<span class="image main"><img src="images/FedTimeSeries/FTS12.png" alt="" /></span>											
							<p>From the confirmatory data analysis, various parameters of the models were checked with the AIC criterion and it was found that the ideal SARIMA model for this data set would be a SARIMA (4,1,4)  X (2,1,2)<sub>12</sub>. The model’s residual RACF was then graphed, and it was found that there was no significant correlation detected. Meaning the residuals are independent and white. The residuals also passed the normality testing by using the normal probability plot of the residuals. The model was also assumed to have passed the constant variance test and was ready for the forecasting and simulations phase.</p>
							<p>The trend forecast can now be used to compare one’s own salary projection to see how they match up to a government worker’s projection, who in theory, should not be getting salary bumps other than reasons of inflation and rise in cost of living. Thus, if one’s salary projection is less than that of a government worker’s, it might indicate that they are not getting the compensation they deserve.  </p>
						</article>
						<!-- KoreanNLP -->
						<article id="ImageColorization">	
							<h2 class="major">Synopsis</h2>	
							<p>In the past few years, instant image colorization has been made widely accessible and popular online. Before, it would take digital artists numerous hours to convert a black and white photo into a colored one. The quality is still more lacking than if a human artist took hours to retouch it; however, the fact that it takes a DL model almost instantly to colorize an image is a huge merit. However, since the release of the publication ‘Colorful Image Colorization’ (R. Zhang, P. Isola, A. A. Efros), rapid BW image conversion was made possible and has been developing even more since then. Even ‘retouched’ movies where old black and white films are colorized have seemingly became more widespread and commonly released ever since this paper was released. </p>
							<p>Since I have started ML projects, I have been thinking of doing a CNN project where I felt like I could generally use for my own personal sake. I started digging/searching for ideas but couldn’t decide what project to try out until I saw some old photos my grandma had with her family. She cherished the photos and I wanted to let her enjoy the photos – now with color. Although I could use an online image color converted, I thought it would be much more meaningful if it was with a model/script that I have done myself. I have initially built my model; however, due to limitations in the dataset and computing time, the results were not to my liking. After digging further through – I realized that even the most advanced, well-trained model still had some deficiencies and I decided to treat my experience as a learning one. There are also models such as Caffe that already exist where I can apply transfer learning, but I just wanted to create my own model and train it myself for educational purposes. </p>
							<h2 class="major">Background</h2>	
							<p>Black and white images only require 1 channel containing a value between 0-255 for each pixel. For color images, its pixels are stored in RGB components. The 3 color channels are added for final colorized image. To convert a black and white image into a colored one, each pixel value would need be componentized into each of the 3 RGB channels. </p>
							<p>The core idea of image colorization utilizes using another color space that is not RGB to extract color feature/characteristic from the original BW image. Lab color space was the proposed and utilized for that purpose. To emulate the work done by Zhang el al., the Lab color space must be used. Lab color is thought to do a good job of how humans perceive color. </p>
							<p>Lab color space has 3 channels like RGB color space, but the channels are different. L channel is for lightness intensity, a channel is for green-red, and b channel is for blue-yellow. The L channel deals with the BW information from the input BW image. The a and b channel are responsible for the colors; thus, the model will be used to train/predict a and b channels. The combined parameters for L, a, and b channels are used to generate the output RGB image. </p>
							<h2 class="major">Dataset</h2>
							<p>For my initial model, the images were pre-processed to be 256 x 256 pixels. Also, the value of the pixels were checked to see if they were in the range of 0-255 as per typical image recognition applications. If the pixel value of the image was out of range, they were made sure to be normalized. The dataset and original framework of the model was from/inspired by https://github.com/guilbera/colorizing and https://github.com/emilwallner/Coloring-greyscale-images </p>	
							<p>The core idea of the model is to train it so that it knows how to colorize/learn features based on its Lab gradient representation. As such, the initial training images were converted into Lab color scheme and the testing black/white images were also converted into Lab. The downloaded images were called into a training array, normalized, and inputted into my model. </p>
							<span class="image main"><img src="images/BW Colorization/BW01.png" alt="" /></span>	
							<p>The model was inputted with the images transformed and converted into Lab color scheme as discussed earlier. The resultant model was used on the test dataset to evaluate its accuracy. A caveat is that the accuracy is based in the Lab space so the low accuracy number may not translate into a well colorizing model. </p>										
							<span class="image main"><img src="images/BW Colorization/BW02.png" alt="" /></span>	
							<h2 class="major">Summary of Dataset</h2>
							<p>The model showed extremely minimal loss. However, the resultant image was not satisfactory due the lack of images/computing power as mentioned in the synopsis. When I started this project, I had severely underestimated how much images/data I would require getting a satisfactory coloring. This statement was reflected by the github contributors I mentioned prior. </p>
							<span class="image main"><img src="images/BW Colorization/BW03.png" alt="" /></span>	
							<span class="image main"><img src="images/BW Colorization/BW04.png" alt="" /></span>	
							<p>In the above image, the model was unable to fully color the initial BW image. However, it had started to pick up characteristics/features where it started to detect/identify humans in the center of the image. Outlines of the hair and patches of skin was showing shades of the correct color. The overall blacks such as the dress were not touched and was remained as the correct color. </p>
							<p>I believe once the model is able to be trained on a wider range of sets using a good computational machine, the results will improve. I am looking forward to continuing my efforts here soon. </p>

						</article>
						<!-- Contact -->
							<article id="contact">
								<h2 class="major">Contact</h2>
								<p>If you are curious about the details of my projects or just have general questions, feel free to reach out to me whenever! My email, linkedin, and github can be found below</p>
								<ul class="icons">
									<li><a href="mailto: sonkelvinj@gmail.com" class="icon brands fa-google"><span class="label">Email</span></a></li>
									<li><a href="https://www.linkedin.com/in/kelvin-son-65438899/" class="icon brands fa-linkedin"><span class="label">Linkedin/span></a></li>
									<li><a href="https://github.com/KelvinJSon" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</article>

						<!-- Elements -->
							<article id="elements">
								<h2 class="major">Elements</h2>

								<section>
									<h3 class="major">Text</h3>
									<p>This is <b>bold</b> and this is <strong>strong</strong>. This is <i>italic</i> and this is <em>emphasized</em>.
									This is <sup>superscript</sup> text and this is <sub>subscript</sub> text.
									This is <u>underlined</u> and this is code: <code>for (;;) { ... }</code>. Finally, <a href="#">this is a link</a>.</p>
									<hr />
									<h2>Heading Level 2</h2>
									<h3>Heading Level 3</h3>
									<h4>Heading Level 4</h4>
									<h5>Heading Level 5</h5>
									<h6>Heading Level 6</h6>
									<hr />
									<h4>Blockquote</h4>
									<blockquote>Fringilla nisl. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan faucibus. Vestibulum ante ipsum primis in faucibus lorem ipsum dolor sit amet nullam adipiscing eu felis.</blockquote>
									<h4>Preformatted</h4>
									<pre><code>i = 0;

while (!deck.isInOrder()) {
    print 'Iteration ' + i;
    deck.shuffle();
    i++;
}

print 'It took ' + i + ' iterations to sort the deck.';</code></pre>
								</section>

								<section>
									<h3 class="major">Lists</h3>

									<h4>Unordered</h4>
									<ul>
										<li>Dolor pulvinar etiam.</li>
										<li>Sagittis adipiscing.</li>
										<li>Felis enim feugiat.</li>
									</ul>

									<h4>Alternate</h4>
									<ul class="alt">
										<li>Dolor pulvinar etiam.</li>
										<li>Sagittis adipiscing.</li>
										<li>Felis enim feugiat.</li>
									</ul>

									<h4>Ordered</h4>
									<ol>
										<li>Dolor pulvinar etiam.</li>
										<li>Etiam vel felis viverra.</li>
										<li>Felis enim feugiat.</li>
										<li>Dolor pulvinar etiam.</li>
										<li>Etiam vel felis lorem.</li>
										<li>Felis enim et feugiat.</li>
									</ol>
									<h4>Icons</h4>
									<ul class="icons">
										<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
										<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
										<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
										<li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
									</ul>

									<h4>Actions</h4>
									<ul class="actions">
										<li><a href="#" class="button primary">Default</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
									<ul class="actions stacked">
										<li><a href="#" class="button primary">Default</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
								</section>

								<section>
									<h3 class="major">Table</h3>
									<h4>Default</h4>
									<div class="table-wrapper">
										<table>
											<thead>
												<tr>
													<th>Name</th>
													<th>Description</th>
													<th>Price</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>Item One</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Two</td>
													<td>Vis ac commodo adipiscing arcu aliquet.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Three</td>
													<td> Morbi faucibus arcu accumsan lorem.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Four</td>
													<td>Vitae integer tempus condimentum.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Five</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
											</tbody>
											<tfoot>
												<tr>
													<td colspan="2"></td>
													<td>100.00</td>
												</tr>
											</tfoot>
										</table>
									</div>

									<h4>Alternate</h4>
									<div class="table-wrapper">
										<table class="alt">
											<thead>
												<tr>
													<th>Name</th>
													<th>Description</th>
													<th>Price</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>Item One</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Two</td>
													<td>Vis ac commodo adipiscing arcu aliquet.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Three</td>
													<td> Morbi faucibus arcu accumsan lorem.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Four</td>
													<td>Vitae integer tempus condimentum.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Five</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
											</tbody>
											<tfoot>
												<tr>
													<td colspan="2"></td>
													<td>100.00</td>
												</tr>
											</tfoot>
										</table>
									</div>
								</section>

								<section>
									<h3 class="major">Buttons</h3>
									<ul class="actions">
										<li><a href="#" class="button primary">Primary</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
									<ul class="actions">
										<li><a href="#" class="button">Default</a></li>
										<li><a href="#" class="button small">Small</a></li>
									</ul>
									<ul class="actions">
										<li><a href="#" class="button primary icon solid fa-download">Icon</a></li>
										<li><a href="#" class="button icon solid fa-download">Icon</a></li>
									</ul>
									<ul class="actions">
										<li><span class="button primary disabled">Disabled</span></li>
										<li><span class="button disabled">Disabled</span></li>
									</ul>
								</section>

								<section>
									<h3 class="major">Form</h3>
									<form method="post" action="#">
										<div class="fields">
											<div class="field half">
												<label for="demo-name">Name</label>
												<input type="text" name="demo-name" id="demo-name" value="" placeholder="Jane Doe" />
											</div>
											<div class="field half">
												<label for="demo-email">Email</label>
												<input type="email" name="demo-email" id="demo-email" value="" placeholder="jane@untitled.tld" />
											</div>
											<div class="field">
												<label for="demo-category">Category</label>
												<select name="demo-category" id="demo-category">
													<option value="">-</option>
													<option value="1">Manufacturing</option>
													<option value="1">Shipping</option>
													<option value="1">Administration</option>
													<option value="1">Human Resources</option>
												</select>
											</div>
											<div class="field half">
												<input type="radio" id="demo-priority-low" name="demo-priority" checked>
												<label for="demo-priority-low">Low</label>
											</div>
											<div class="field half">
												<input type="radio" id="demo-priority-high" name="demo-priority">
												<label for="demo-priority-high">High</label>
											</div>
											<div class="field half">
												<input type="checkbox" id="demo-copy" name="demo-copy">
												<label for="demo-copy">Email me a copy</label>
											</div>
											<div class="field half">
												<input type="checkbox" id="demo-human" name="demo-human" checked>
												<label for="demo-human">Not a robot</label>
											</div>
											<div class="field">
												<label for="demo-message">Message</label>
												<textarea name="demo-message" id="demo-message" placeholder="Enter your message" rows="6"></textarea>
											</div>
										</div>
										<ul class="actions">
											<li><input type="submit" value="Send Message" class="primary" /></li>
											<li><input type="reset" value="Reset" /></li>
										</ul>
									</form>
								</section>

							</article>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">Kelvin J. Son</p>
						<p class="copyright"> Design: <a href="https://html5up.net">HTML5 UP</a></p>
					</footer>

			</div>

		<!-- BG -->
			<div id="bg"></div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
